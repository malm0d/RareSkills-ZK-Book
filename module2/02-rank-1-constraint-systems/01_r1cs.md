# Converting Algebraic Circuits to R1CS
https://www.rareskills.io/post/rank-1-constraint-system

## Prerequisites
- Arithmetic circuits
- Modular arithmetic
- All operations here happen in a finite field, so $-5$ is the additive inverse of $5 \ (\text{mod}\ p)$; and $2/3$ is the multiplicative inverse of $3 \ (\text{mod}\ p)$ times $2$.

## R1CS Overview
A Rank 1 Constraint System (R1CS) is an arithmetic circuit with the requirement that each equality constraint has one multiplication (and no restriction on the number of additions). 

That is, each constraint in an R1CS (each line in the system of equations) enforces exactly ONE multiplication between two linear combinations of variables (e.g. $x \cdot x$, or $x \cdot y$, etc), with no restrictions on the number of additions within those linear combinations or across the system of equations.

This makes the representation of the arithmetic circuit compatible with the use of bilinear pairings. The output of a bilinear pairing: $e: G_1 \bullet G_2 \rightarrow G_T$ cannot be paired again, as an element in $G_T$ cannot be used as part of the input of another bilinear pairing ($G_T$ is a multiplicative subgroup of a finte field, while $G_1$ and $G_2$ are elliptic curve groups). As such, we only allow one multiplication per constraint.

## The Witness Vector
In an arithmetic circuit, the witness is an assignment of values to all signals (variables) such that all constraints (equations) in the circuit are satisfied.

In an R1CS, the witness is a $1 \times n$ vector that contains the values of all input variables, output variables, and intermediate variables. It demonstrates that we have executed the circuit from start to finish, knowing all the input, output, and intermediate values.

By convention, the first element is always: $1$, 

This helps to make some calculations easier (more later).

For example, if we have the constraint:

$$
z = x^{2}y
$$

and we claim to know the solution for the constraint, then it means that we know: $x$, $y$, and $z$.

Because R1CS require exactly one multiplication per constraint, the above polynomial constraint above must broken down into simpler constraints, each with only ONE multiplication:

$$
v_1 = xx \\
z = v_1y
$$

A witness means that we don't just know $x$, $y$, and $z$, it also means we must also know every intermediate variable in the system of equations. Thus, our witness is the vector:

$$
[1, z, x, y, v_1]
$$

where each term has a value that satisfies the constraints above.

For example:

$$
[1, 18, 3, 2, 9]
$$

is a valid witness because when we pass the values into the circuit

$$
[constant = 1, \ z = 18, \ x = 3, \ y = 2, \ v_1 = 9]
$$

it satisfies the constraints

$$
\begin{align*}
v_1 = x \ast x \quad &\rightarrow \quad 9 = 3 \ast 3 \\
z = v_1 \ast y \quad &\rightarrow \quad 18 = 9 \ast 2
\end{align*}
$$

Note that the $1$ term in the witness vector is not used in this example, and it is a convenience that will be explained later on.

## Example 1: Transforming $z = xy$ into a R1CS
For this example, say we are proving: $4223 = 41 \times 103$.

Therrefore, the witness vector: $[1, z, x, y]$ will be assigned: $[1, 4223, 41, 103]$.

Before we can create an R1CS, the constraints need to be of the form:

$$
\text{result} = \text{LHS} \times \text{RHS}
$$

That is:

$$
\underbrace{z}_\text{result} = \underbrace{x}_\text{LHS} \times \underbrace{y}_\text{RHS}
$$

This is a trivial example. And as a reminder, to create a valid R1CS, we need a system of equations where each constraint has exactly ONE multiplication.


### System of Equations in the form: $\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}$
The goal is to create a system of equations of the form:

$$
\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}
$$

Where: $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ are MATRICES of size: $n \times m$ (i.e. $n$ rows, and $m$ columns).

In the system of equations that make up the circuit:
- The vector $\mathbf{a}$ is the witness vector.
- Matrix $\mathbf{O}$ encodes all the result variables in the system of equations.
- Matrix $\mathbf{L}$ encodes all the LHS variables in the system of equations.
- Matrix $\mathbf{R}$ encodes all the RHS variables in the system of equations.

Importantly, matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ have the SAME NUMBER OF COLUMNS as the witness vector $\mathbf{a}$. And, each column (in both the matrices and the witness vector) represents the same variable the index is using.

Linking back to example 1, the witness has 4 elements: $[1, z, x, y]$, which means each of the matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ will have 4 columns (i.e. $m = 4$). And an index of $1$ for instance, would refer to the variable $z$ across the matrices and the witness vector.

The number of rows in each of the matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ will correspond to the number of constraints in the circuit (i.e. the number of equations in the system of equations) In example 1, as there is only one constraint: $z = xy$, then there will only be one row in each of the matrices (i.e. $n = 1$).

The R1CS for example 1 will be:

$$
z = xy
$$

$$
\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}
$$

$$
\underbrace{\begin{bmatrix}
0 & 1 & 0 & 0 \\
\end{bmatrix}}_{\mathbf{O}}\mathbf{a} =
\underbrace{\begin{bmatrix}
0 & 0 & 1 & 0 \\
\end{bmatrix}}_{\mathbf{L}}\mathbf{a} \circ
\underbrace{\begin{bmatrix}
0 & 0 & 0 & 1 \\
\end{bmatrix}}_{\mathbf{R}}\mathbf{a}
$$

$$
\begin{bmatrix}0 & 1 & 0 & 0\end{bmatrix}
\begin{bmatrix}1 \\ 4223 \\ 41 \\ 103 \\ \end{bmatrix} =
\begin{bmatrix}0 & 0 & 1 & 0 \\ \end{bmatrix}
\begin{bmatrix}1 \\ 4223 \\ 41 \\103 \\ \end{bmatrix} \circ
\begin{bmatrix}0 & 0 & 0 & 1 \\ \end{bmatrix}
\begin{bmatrix}1 \\ 4223 \\ 41 \\ 103 \\ \end{bmatrix}
$$

Each item in matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ serves an an "indicator" for whether or not the variable that the column corresponds with is present. (Technically, it is the coefficient of the variable, but more on this in later examples).

Note that the multiplication between each matrix ($\mathbf{O}$, $\mathbf{L}$, $\mathbf{R}$) and the witness vector ($\mathbf{a}$) is a standard matrix multiplication operation. However, the operation: $\circ$ is a hadamard product (element-wise multipication).

The hadamard product between $\mathbf{La}$ and $\mathbf{Ra}$ enforces the one multiplication per constraint requirement in R1CS.

Given the witness $a$, the columns represent $[1, z, x, y]$, then:

- $ \mathbf{L}$ is $[0,0,1,0]$ because $x$ is present and it is the only variable on the LHS of the multiplication.

- $ \mathbf{R}$ is $[0,0,0,1]$ because $y$ is present and it is the only variable on the RHS of the multiplication.

- $ \mathbf{O}$ is $[0,1,0,0]$ because $z$ is the only variable in the "output" of the multiplication.

- There is no constant anywhere in the system of equations, and so the column for $1$ is zero in $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ (more on when it is non-zero later).

We can verify that the equation is correct, in Python:
```python
import numpy as np

# Matrices: O, L, R
O = np.matrix([[0,1,0,0]])
L = np.matrix([[0,0,1,0]])
R = np.matrix([[0,0,0,1]])

# Witness vector: a
a = np.array([1, 4223, 41, 103])

# The multiplication `*` is element-wise (hadamard product),
# not the standard matrix multiplication. But `np.matmul` is
# the standard matrix multiplication operation.
#
# Result contains a bool indicating an element-wise indicator that
# the equality is true for that element. I.e. `==` compares each
# entry on the left to the corresponding entry on the right, and
# produces a boolean at the same position
result = np.matmul(O, a) == np.matmul(L, a) * np.matmul(R, a)

# Check that every element-wise equality is true.
# Collapses the boolean array to a single truth value,
# returning `True` only when very entry is `True`.
assert result.all(), "result contains an inequality"
```

It would be correct to say that we are just doing $41 \times 103 = 4223$ in a very verbose way. R1CS can be very verbose, but they map nicely to [Quadratic Arithmetic Programs (QAPs)](https://www.rareskills.io/post/quadratic-arithmetic-program), which can be made succint. QAPs are out of scope in this chapter.

However, this is an important point of R1CS. A R1CS communicates exactly the same information as the original arithmetic constraints, but with only ONE multiplication per equality constraint. In example 1, we only have one constraint, but we will add more in the next example.

## Example 2: Transforming $r = xyzu$ into a R1CS
This example shows how to deal with intermediate variables. If we consider the equation:

$$
r = xyzu
$$

This clearly has more than one multiplication operation in a single computation.

In an R1CS, each row of computation (each constraint) can only have ONE multipication, so the above equation needs to be broken up:

$$
\begin{align*}
v_1 &= xy \\
v_2 &= zu \\
r &= v_1v_2
\end{align*}
$$

Note that there is no fixed rule to say how we need to break up the original equation. The following is also valid:

$$
\begin{align*}
v_1 &= xy \\
v_2 &= v_1z \\
r &= v_2u
\end{align*}
$$

We will use the first transformation for this example.

### Size of $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$
Remember what was said about the witness vector $a$ and number of columns and rows in matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$.

Since we are dealing with $7$ variables in the system of equations: ($r$, $x$, $y$, $z$, $u$, $v_1$, $v_2$), the witness vector will have $8$ elements (the first being the additional constant $1$); and each of the matrices will have $8$ columns.

Since we now have $3$ constraints in the R1CS, then each of the matrices will have $3$ rows.

### Left Hand Terms (LHS) and Right Hand Terms (RHS)
In the system of linear equations, when we say "left hand terms" we are referring to all terms on the left hand side (LHS) of the multiplication in each constraint; and when we say "right hand terms" we are referring to all terms on the right hand side (RHS) of the multiplication in each constraint. That is, in:

$$
\underbrace{
    \begin{matrix}
        v_1 \\
        v_2 \\
        r \\
    \end{matrix}
}_\text{ Output Terms }

\begin{matrix}
=\\
=\\
=
\end{matrix}

\underbrace{
    \begin{matrix}
        x \\
        z \\
        v_1 \\
    \end{matrix}
}_\text{ Left Hand Terms }

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}

\underbrace{
    \begin{matrix}
        y \\
        u \\
        v_2 \\
    \end{matrix}
}_\text{ Right Hand Terms }
$$

Specifically:

- Left hand terms: $x$, $z$, $v_1$ 

- Right hand terms: $y$, $u$, $v_2$

### Constructing Matrix $\mathbf{L}$ from Left Hand Terms
We know matrix $\mathbf{L}$ will have $3$ rows (since there are $3$ constraints) and $8$ columns (since there are $8$ elements in the witness).

$$
\mathbf{L} = 
\begin{bmatrix}
l_{1,1} & l_{1,2} & l_{1,3} & l_{1,4} & l_{1,5} & l_{1,6} &  l_{1,7} & l_{1,8} \\
l_{2,1} & l_{2,2} & l_{2,3} & l_{2,4} & l_{2,5} & l_{2,6} &  l_{2,7} & l_{2,8} \\
l_{3,1} & l_{3,2} & l_{3,3} & l_{3,4} & l_{3,5} & l_{3,6} &  l_{3,7} & l_{3,8} \\
\end{bmatrix}
$$

Note that at this point, for $l_{n,m}$ in the matrix $\mathbf{L}$, $n$ refers to the row and $m$ refers to the column.

The witness vector $\mathbf{a}$ will be multiplied (standard matrix multiplication) by matrix $\mathbf{L}$. Based on the system of equations, the witness vector has the following layout:

$$
\mathbf{a} =
\begin{bmatrix}
1 & r &x & y & z & u & v_1 & v_2
\end{bmatrix}
$$

We can rewrite the matrix $\mathbf{L}$ so we know what variable (in the wtiness) each column is representing:

$$
\mathbf{L} = 
\begin{bmatrix}
l_{1, 1} & l_{1, r} & l_{1, x} & l_{1, y} & l_{1, z} & l_{1, u} & l_{1, v_1} & l_{1, v_2} \\
l_{2, 1} & l_{2, r} & l_{2, x} & l_{2, y} & l_{2, z} & l_{2, u} & l_{2, v_1} & l_{2, v_2} \\
l_{3, 1} & l_{3, r} & l_{3, x} & l_{3, y} & l_{3, z} & l_{3, u} & l_{3, v_1} & l_{3, v_2} \\
\end{bmatrix}
$$

Now, for $l_{n,m}$ in the matrix $\mathbf{L}$, $n$ refers to the row and $m$ refers to the variable in the witness vector that the column is representing.

### First Row of Matrix $\mathbf{L}$
Recall that the system of equations in the R1CS is:

$$
\begin{align*}
v_1 &= xy \\
v_2 &= zu \\
r &= v_1v_2
\end{align*}
$$

The first row is: $v_1 = xy$. And for the first left (LHS) variable, we have:

$$
\begin{matrix}
    v_1 \\
    v_2 \\
    r \\
\end{matrix}

\begin{matrix}
=\\
=\\
=
\end{matrix}

\underset{\mathbf{L}}{\boxed{
    \begin{matrix}
        \color{red}{x} \\
        z \\
        v_1 \\
    \end{matrix}
}}

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}

\begin{matrix}
    y \\
    u \\
    v_2 \\
\end{matrix}
$$


This means, in the first row with respect to the LHS, only the variable $x$ is present, and no other variable is present. Therefore, we can transform the first row of matrix $\mathbf{L}$ to the following:

$$
\mathbf{a} =
\begin{bmatrix}
1 & r &x & y & z & u & v_1 & v_2
\end{bmatrix}
$$

$$
\mathbf{L}=\begin{bmatrix}
0 & 0 & \color{red}{1} & 0 & 0 & 0 & 0 & 0 \\
l_{2, 1} & l_{2, r} & l_{2, x} & l_{2, y} & l_{2, z} & l_{2, u} & l_{2, v_1} & l_{2, v_2} \\
l_{3, 1} & l_{3, r} & l_{3, x} & l_{3, y} & l_{3, z} & l_{3, u} & l_{3, v_1} & l_{3, v_2} \\
\end{bmatrix}
$$

We can see that for the first row, only $1$ is in the $x$ column, indicating that in the system of equations, in the first constraint, $x$ is the only left hand term - which is true.

### Second Row of Matrix $\mathbf{L}$
In the second row of the system of equations in the R1CS: $v_2 = zu$, we see that $z$ is the only variable present on the LHS:

$$
\begin{matrix}
    v_1 \\
    v_2 \\
    r \\
\end{matrix}

\begin{matrix}
=\\
=\\
=
\end{matrix}

\underset{\mathbf{L}}{\boxed{
    \begin{matrix}
        x \\
        \color{green}{z} \\
        v_1 \\
    \end{matrix}
}}

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}

\begin{matrix}
    y \\
    u \\
    v_2 \\
\end{matrix}
$$

Therefore we can transform the second row of matrix $\mathbf{L}$ by setting everything to $0$ except for the column that represents the $z$ variable:

$$
\mathbf{L}=\begin{bmatrix}
0 & 0 & \color{red}{1} & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \color{green}{1} & 0 & 0 & 0 \\
l_{3, 1} & l_{3, r} & l_{3, x} & l_{3, y} & l_{3, z} & l_{3, u} & l_{3, v_1} & l_{3, v_2} \\
\end{bmatrix}
$$

### Third Row of Matrix $\mathbf{L}$
In the third row of the system of equations in the R1CS: $r = v_1v_2$, we have $v_1$ as the only variable present on the LHS:

$$
\begin{matrix}
    v_1 \\
    v_2 \\
    r \\
\end{matrix}

\begin{matrix}
=\\
=\\
=
\end{matrix}

\underset{\mathbf{L}}{\boxed{
    \begin{matrix}
        x \\
        z \\
        \color{violet}{v_1} \\
    \end{matrix}
}}

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}

\begin{matrix}
    y \\
    u \\
    v_2 \\
\end{matrix}
$$

We can transform the third row of matrix $\mathbf{L}$ by setting everything to $0$ except for the column that represents the $v_1$ variable, and this completes matrix $\mathbf{L}$:

$$
\mathbf{L}=\begin{bmatrix}
0 & 0 & \color{red}{1} & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \color{green}{1} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \color{violet}{1} & 0 \\
\end{bmatrix}
$$

The following shows the full mapping:

$$
\begin{matrix}
    v_1 \\
    v_2 \\
    r \\
\end{matrix}
\begin{matrix}
=\\
=\\
=
\end{matrix}
\underset{\mathbf{L}}{\boxed{
    \begin{matrix}
        \color{red}{x} \\
        \color{green}{z} \\
        \color{violet}{v_1} \\
    \end{matrix}
}}
\begin{matrix}
\times\\
\times\\
\times
\end{matrix}
    \begin{matrix}
        y \\
        u \\
        v_2 \\
    \end{matrix}

\space\space\space\space

\begin{array}{c}
\begin{array}{cc}
\begin{matrix}
1 & r & x & y & z & u & v_1 & v_2 \\
\end{matrix}
\end{array} \\[5pt]

\begin{array}{cc}
\begin{bmatrix}
0 & 0 & \color{red}{1} & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \color{green}{1} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \color{violet}{1} & 0 \\
\end{bmatrix}
\end{array}
\end{array}
$$

### Alternative Transformation of Matrix $\mathbf{L}$
The left hand terms in the R1CS can technically be expanded as the following:

$$
\begin{align*}
v_1 &= xy \\
v_2 &= zu \\
r &= v_1v_2
\end{align*}
$$

$$
\begin{align*}
v_1 &= (0\cdot 1 + 0\cdot r + \boxed{1\cdot x} + 0\cdot y + 0\cdot z + 0\cdot u + 0\cdot v_1 + 0\cdot v_2) \times y\\
v_2 &= (0\cdot 1 + 0\cdot r + 0\cdot x + 0\cdot y + \boxed{1\cdot z} + 0\cdot u + 0\cdot v_1 + 0\cdot v_2) \times u\\
r &= (0\cdot 1 + 0\cdot r + 0\cdot x + 0\cdot y + 0\cdot z + 0\cdot u + \boxed{1\cdot v_1} + 0\cdot v_2) \times v_2 \\ 
\end{align*}
$$

Adding terms that are $0$ does not change the values. We just have to be careful during the expansion of the zero variables to match the columns in the witness vector $\mathbf{a}$.

And if we take the coefficients (shown in boxes) out of the above expansion:

$$
\begin{align*}
v_1 &= (\boxed{0}\cdot 1 + \boxed{0}\cdot r + \boxed{1}\cdot x + \boxed{0}\cdot y + \boxed{0}\cdot z + \boxed{0}\cdot u + \boxed{0}\cdot v_1 + \boxed{0}\cdot v_2) \times y\\
v_2 &= (\boxed{0}\cdot 1 + \boxed{0}\cdot r + \boxed{0}\cdot x + \boxed{0}\cdot y + \boxed{1}\cdot z + \boxed{0}\cdot u + \boxed{0}\cdot v_1 + \boxed{0}\cdot v_2) \times u\\
r &= (\boxed{0}\cdot 1 + \boxed{0}\cdot r + \boxed{0}\cdot x + \boxed{0}\cdot y + \boxed{0}\cdot z + \boxed{0}\cdot u + \boxed{1}\cdot v_1 + \boxed{0}\cdot v_2) \times v_2 \\ 
\end{align*}
$$

We get the same matrix $\mathbf{L}$ we arrived at previously:

$$
\mathbf{L}=\begin{bmatrix}
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
$$

### Constructing Matrix $\mathbf{R}$ from Right Hand Terms
Like matrix $\mathbf{L}$, we know that matrix $\mathbf{R}$ will also have $3$ rows (since there are $3$ constraints) and $8$ columns (since there are $8$ elements in the witness).

And we also know that the witness vector $\mathbf{a}$ will be multiplied (standard matrix multiplication) by matrix $\mathbf{R}$:

$$
\mathbf{a} =
\begin{bmatrix}
1 & r &x & y & z & u & v_1 & v_2
\end{bmatrix}
$$

We can rewrite the matrix $\mathbf{R}$ so we know what variable (in the wtiness) each column is representing:

$$
R = \begin{bmatrix}
r_{1,1} & r_{1,r} & r_{1,x} & r_{1,y} & r_{1,z} & r_{1,u} & r_{1,v_1} & r_{1,v_2} \\
r_{2,1} & r_{2,r} & r_{2,x} & r_{2,y} & r_{2,z} & r_{2,u} & r_{2,v_1} & r_{2,v_2} \\
r_{3,1} & r_{3,r} & r_{3,x} & r_{3,y} & r_{3,z} & r_{3,u} & r_{3,v_1} & r_{3,v_2} \\
\end{bmatrix}
$$

Now, for $r_{n,m}$ in the matrix $\mathbf{R}$, $n$ refers to the row and $m$ refers to the variable in the witness vector that the column is representing.

Matrix $\mathbf{R}$ represents the right hand terms ($y$, $u$, $v_2$) in the system of equations of the R1CS:

$$
\begin{matrix}
    v_1 \\
    v_2 \\
    r \\
\end{matrix}

\begin{matrix}
=\\
=\\
=
\end{matrix}

{
    \begin{matrix}
        x \\
        z \\
        v_1 \\
    \end{matrix}
}

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}

\underset{\mathbf{R}}{\boxed{
\begin{matrix}
    \color{red}y \\
    \color{green}u \\
    \color{violet}v_2 \\
\end{matrix}}}
$$

As each row in the matrix represents each constraint in the system of equations, matrix $\mathbf{R}$ must have the value $1$ in the columns that represent the right hand terms ($y$, $u$, $v_2$). That is in the matrix, the first row has $1$ in the $y$ column, the second row has $1$ in the $u$ column, and the third row has $1$ in the $v_2$ column:

$$
\mathbf{a} =
\begin{bmatrix}
1 & r &x & y & z & u & v_1 & v_2
\end{bmatrix}
$$

$$
\mathbf{R}=\begin{bmatrix}
0 & 0 & 0 & \color{red}{1} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & \color{green}{1} & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \color{violet}{1} \\
\end{bmatrix}
$$

The following shows the full mapping:

$$
\begin{matrix}
    v_1 \\
    v_2 \\
    r \\
\end{matrix}
\begin{matrix}
=\\
=\\
=
\end{matrix}

    \begin{matrix}
        x \\
        z \\
        v_1 \\
    \end{matrix}

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}
\underset{\mathbf{R}}
{\boxed{
    \begin{matrix}
        \color{red}{y} \\
        \color{green}{u} \\
        \color{violet}{v_2} \\
    \end{matrix}
}}

\space\space\space\space

\begin{array}{c}
\begin{array}{cc}
\begin{matrix}
1 & r & x & y & z & u & v_1 & v_2 \\
\end{matrix}
\end{array} \\[5pt]

\begin{array}{cc}
\begin{bmatrix}
0 & 0 & 0 & \color{red}{1} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & \color{green}{1} & 0 & 0  \\
0 & 0 & 0 & 0 & 0 & 0 & 0 &\color{violet}{1}  \\
\end{bmatrix}
\end{array}
\end{array}
$$

### Constructing Matrix $\mathbf{O}$ from Output Terms
Matrix $\mathbf{O}$ represents the output terms ($v_1$, $v_2$, $r$) in the system of equations of the R1CS:

$$
\underset{\mathbf{O}}{\boxed{
\begin{matrix}
    \color{red}{v_1} \\
    \color{green}{v_2} \\
    \color{violet}{r} \\
\end{matrix}}}

\begin{matrix}
=\\
=\\
=
\end{matrix}

\begin{matrix}
    x \\
    z \\
    v_1 \\
\end{matrix}

\begin{matrix}
\times\\
\times\\
\times
\end{matrix}

\begin{matrix}
    y \\
    u \\
    v_2 \\
\end{matrix}
$$

Like matrix $\mathbf{L}$ and $\mathbf{R}$, we know that matrix $\mathbf{O}$ will also have $3$ rows (since there are $3$ constraints) and $8$ columns (since there are $8$ elements in the witness). Knowing what we know, matrix O is represented as follows:

$$
\mathbf{a} =
\begin{bmatrix}
1 & r &x & y & z & u & v_1 & v_2
\end{bmatrix}
$$

$$
\mathbf{O}=\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 & \color{red}{1} & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \color{green}{1}  \\
0 & \color{violet}{1} & 0 & 0 & 0 & 0 & 0 & 0  \\
\end{bmatrix}
$$

### Checking for $r = xyzu$
In Python:
```python
import numpy as np

# enter the L, R and O from above
L = np.matrix([[0,0,1,0,0,0,0,0],
              [0,0,0,0,1,0,0,0],
              [0,0,0,0,0,0,1,0]])

R = np.matrix([[0,0,0,1,0,0,0,0],
              [0,0,0,0,0,1,0,0],
              [0,0,0,0,0,0,0,1]])

O = np.matrix([[0,0,0,0,0,0,1,0],
              [0,0,0,0,0,0,0,1],
              [0,1,0,0,0,0,0,0]])

# random values for x, y, z, and u (the inputs)
import random
x = random.randint(1,1000)
y = random.randint(1,1000)
z = random.randint(1,1000)
u = random.randint(1,1000)

# compute the algebraic circuit
r = x * y * z * u
v1 = x*y
v2 = z*u

# create the witness vector
a = np.array([1, r, x, y, z, u, v1, v2])

# Oa, La, and Ra are obtained by standard matrix multiplication,
# but the result is calculated by element-wise multiplication 
# (hadamard product) of La and Ra, not standard matrix multiplication
result = np.matmul(O, a) == np.multiply(np.matmul(L, a), np.matmul(R, a))

# Check that every element-wise equality is true.
# Collapses the boolean array to a single truth value,
# returning `True` only when very entry is `True`.
assert result.all(), "system contains an inequality"
```

## Example 3: Addition with a Constant - Transforming $z=xy+k$ into a R1CS
Suppose we want to build an R1CS for:

$$
z = xy + 2
$$

Recall from earlier in the chapter that the first element in the witness vector is always: $1$. This is where the $1$ column comes in handy.

### "Addition is free"
The statement "addition is free" is usually used in the context of ZK-SNARKs. What this means is: we DO NOT have to create an additional constraint when we have an addition operation.

We could potentially write the above to the following:

$$
v_1 = xy \\
z = v_1 + 2
$$

but this would make the R1CS larger than it needs to be. Instead, we can rewrite the above as the following:

$$
-2 \ + \ z = xy
$$

The variable $z$ and the constant $-2$ are automatically "combined" when we multiply the witness vector $\mathbf{a}$ with the matrix $\mathbf{O}$

The witness vector in this example will have $4$ variables - each representing a column:

$$
\mathbf{a}=\begin{bmatrix}
1 & z & x & y
\end{bmatrix}
$$

And since the R1CS only has $1$ constraint, our matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ will be $1 \times 4$ matrices.

$$
\begin{align*}
\mathbf{O} &= \begin{bmatrix}
-2 & 1 & 0 & 0 \\
\end{bmatrix} \\
\mathbf{L} &= \begin{bmatrix}
0 & 0 & 1 & 0 \\
\end{bmatrix} \\
\mathbf{R} &= \begin{bmatrix}
0 & 0 & 0 & 1 \\
\end{bmatrix}
\end{align*}
$$

Whenever there are additive constants, we place them in the $1$ column, which by convention is the first column.

### Why Do We Move the Additive Term Away from the Multiplication?
We saw from above that:

$$
z = xy + 2
$$

was rewritten as:

$$
-2 \ + \ z = xy
$$

In R1CS, every constraint in the system of equations MUST have the form:

$$
\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}
$$

Which means a constraint only allows multiplication on the right hand side of the equation, and a linear combination of variables on the left hand side of the equation. That is R1CS only allow one multiplication (via $\circ$), and additions cannot be handled in that operation since it would violate R1CS rules.

Additionally, as shown above, this would mean introducing an extra constraint just to handle the addition of the constant, which is inefficient.

By moving the constant (additive term) away from the multiplication, we allow it to be part of the witness vector via the $1$ variable. Note that this applies to variables as well (not just constants).

To check the work:
```python
import numpy as np
import random

# Define the matrices
O = np.matrix([[-2,1,0,0]])
L = np.matrix([[0,0,1,0]])
R = np.matrix([[0,0,0,1]])

# pick random values to test the equation
x = random.randint(1,1000)
y = random.randint(1,1000)

# witness vector: [1, z, x, y]
z = x * y + 2
a = np.array([1, z, x, y])

# check the equality
result = O.dot(a) == np.multiply(np.matmul(L, a), R.dot(a))
assert result.all(), "result contains an inequality"
```

## Exmaple 4: Multiplication with a Constant - Transforming $z = kx^{n}+y$ into a R1CS
In all of the previous examples, we never multiplied variables by constants, i.e. we never had something like a $kx^{n}$ or a $ky$ term where $k$ is a constant.

That's the reason why in all the R1CS shown so far (except example 3), the entries in their matrices are always just $1$; and matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ only had $1$ s in them.

In example 3, we saw that one of the matrices had an entry that isn't $1$.

Every entry in a matrix in an R1CS is the SAME VALUE OF THE CONSTANT that the corresponding variable is multiplied by.

Consider the solution for:

$$
z = 2x^2 + y
$$

#### "One Multiplication Per Constraint"
What this really means is the MULTIPLICATION OF TWO VARIABLES. Any multiplication with a constant is not considered a "real" multiplication because multiplication with a constant is really just repeated the addition of the same variable.

The following solution is valid but inefficient:

$$
v_1 = xx \\
-y + z = 2v_1
$$

The optimal solution would be:

$$
-y + z = 2xx
$$

In the case, the witness vector $\mathbf{a}$ is:

$$
\mathbf{a} = \begin{bmatrix}
1 & z & x & y
\end{bmatrix}
$$

And the matrices $\mathbf{O}$, $\mathbf{L}$, and $\mathbf{R}$ will be defined as:

$$
\begin{align*}
\mathbf{O} &= \begin{bmatrix}
0 & 1 & 0 & -1 \\
\end{bmatrix} \\
\mathbf{L} &= \begin{bmatrix}
0 & 0 & 2 & 0 \\
\end{bmatrix} \\
\mathbf{R} &= \begin{bmatrix}
0 & 0 & 1 & 0 \\
\end{bmatrix} \\
\end{align*}
$$

When we assemble the matrices and the witness vector into the R1CS, we will get back the original equation:

$$
\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}
$$

$$
\begin{bmatrix}
0 & 1 & 0 & -1 \\
\end{bmatrix}

\begin{bmatrix}
1 \\
z \\
x \\
y \\
\end{bmatrix} =

\begin{bmatrix}
0 & 0 & 2 & 0 \\
\end{bmatrix}

\begin{bmatrix}
1 \\
z \\
x \\
y \\
\end{bmatrix}\circ

\begin{bmatrix}
0 & 0 & 1 & 0 \\
\end{bmatrix}

\begin{bmatrix}
1 \\
z \\
x \\
y \\
\end{bmatrix}
$$